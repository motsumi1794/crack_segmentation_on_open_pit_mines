{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LOADING REQUIRED PACKAGES"
      ],
      "metadata": {
        "id": "H0ksyxoSee7m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdTZj2_oeQjv"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "print('packages installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD A PRETRAINED MODEL"
      ],
      "metadata": {
        "id": "NOdqXq6jeoZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained U-Net model\n",
        "pretrained_model = tf.keras.models.load_model('/Users/tebogoletshwiti/Desktop/unet_model_epoch_5000.h5')\n",
        "\n",
        "# Display the summary of the pre-trained model\n",
        "pretrained_model.summary()"
      ],
      "metadata": {
        "id": "um2UqJ-UelLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SELECT THE FINAL OUTPUT OF THE ENCODER TO INPUT INTO THE DECODER"
      ],
      "metadata": {
        "id": "8css1p9qezXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the output of the last layer in the encoder (decoder input)\n",
        "decoder_input = pretrained_model.get_layer('conv2d_9').output"
      ],
      "metadata": {
        "id": "sXUsxqRPexwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE A NEW DECODER/EXPANSIVE PATH FOR THE MODEL"
      ],
      "metadata": {
        "id": "353Z0H8sfm41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Expansive path\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', name='ex_layer_11')(decoder_input)\n",
        "u6 = tf.keras.layers.concatenate([u6, pretrained_model.get_layer('conv2d_7').output])\n",
        "c6 = tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_12')(u6)\n",
        "c6 = tf.keras.layers.Dropout(0.2, name='do_layer_1')(c6)\n",
        "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_13')(c6)\n",
        "\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2,2), strides=(2, 2), padding='same', name='ex_layer_14')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, pretrained_model.get_layer('conv2d_5').output])\n",
        "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_15')(u7)\n",
        "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_16')(c7)\n",
        "\n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2,2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, pretrained_model.get_layer('conv2d_3').output])\n",
        "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_17')(u8)\n",
        "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_18')(c8)\n",
        "\n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2,2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, pretrained_model.get_layer('conv2d_1').output], axis=3)\n",
        "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_19')(u9)\n",
        "c9 = tf.keras.layers.Dropout(0.1, name='do_layer_2')(c9)\n",
        "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same', name='ex_layer_20')(c9)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid', name='ex_layer_21')(c9)\n",
        "new_model = tf.keras.Model(inputs=pretrained_model.input, outputs=[outputs])\n",
        "new_model.summary()"
      ],
      "metadata": {
        "id": "xl_exITsfXCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDENTIFY THE INDEX OF THE LAYER WHERE YOU WANT TO SPLIT THE PRETRAINED MODEL"
      ],
      "metadata": {
        "id": "YvJm7iQWf3mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the index of the layer where you want to split the pretrained model\n",
        "split_layer_index = new_model.layers.index(pretrained_model.get_layer('conv2d_9'))\n",
        "\n",
        "# Freeze the layers up to the split layer\n",
        "for layer in new_model.layers[:split_layer_index]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "BNpxZdj-ftHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model (adjust based on your task)\n",
        "new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "new_model.summary()"
      ],
      "metadata": {
        "id": "CpR6zACxgIgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD IMAGES AND CREATE THE TRAINING AND TESTING DATASET"
      ],
      "metadata": {
        "id": "c9dZYwPYg5kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder containing all the images and masks\n",
        "data_folder = '/Users/tebogoletshwiti/Desktop/Image_Segmentation'\n",
        "\n",
        "# List of images and masks\n",
        "# image_ids = next(os.walk(data_folder + '/images'))[2]\n",
        "masks_ids = next(os.walk(data_folder + '/masks'))[2]\n",
        "\n",
        "# divide images 80% to training and 20% to testing\n",
        "training_data = []\n",
        "testing_data = image_ids.copy()\n",
        "\n",
        "file1 = open(\"/content/gdrive/MyDrive/Colab Notebooks/training_images.txt\",\"a\")\n",
        "file2 = open(\"/content/gdrive/MyDrive/Colab Notebooks/testing_images.txt\",\"a\")\n",
        "\n",
        "total = 0\n",
        "while total <= (round(len(image_ids) * 0.8)):\n",
        "\n",
        "  index = random.randint(0, len(testing_data) - 1)\n",
        "  training_data.append(image_ids[index])\n",
        "  testing_data.pop(index)\n",
        "\n",
        "  total += 1\n",
        "\n",
        "for image in training_data:\n",
        "  file1.writelines(image + '\\n')\n",
        "\n",
        "for image in testing_data:\n",
        "  file2.writelines(image + '\\n')\n",
        "\n",
        "file1.close()\n",
        "file2.close()"
      ],
      "metadata": {
        "id": "yyfY1hAOgjx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_imgs = []\n",
        "testing_imgs = []\n",
        "\n",
        "data_folder = '/Users/tebogoletshwiti/Desktop/Image_Segmentation/'\n",
        "\n",
        "file1 = open(data_folder + \"training_images.txt\",\"r+\")\n",
        "file2 = open(data_folder + \"testing_images.txt\",\"r+\")\n",
        "\n",
        "for item in file1.readlines():\n",
        "  training_imgs.append(item.strip())\n",
        "\n",
        "for item in file2.readlines():\n",
        "  testing_imgs.append(item.strip())\n",
        "\n",
        "mask_ids = training_imgs.copy()"
      ],
      "metadata": {
        "id": "Mk0QkiKxgTpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 44\n",
        "np.random.seed = seed\n",
        "\n",
        "IMG_WIDTH = 640\n",
        "IMG_HEIGHT = 640\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "X_train = np.zeros((len(training_imgs), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_train = np.zeros((len(training_imgs), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "\n",
        "print('Resizing training images and masks')\n",
        "for n, id_ in tqdm(enumerate(training_imgs), total=len(training_imgs)):\n",
        "\n",
        "    img = cv2.imread(data_folder + 'images/' + id_)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:,:,:IMG_CHANNELS]\n",
        "    X_train[n] = img  #Fill empty X_train with values from img\n",
        "\n",
        "    mask_ = cv2.imread(data_folder + 'masks/' + id_[0:-4] + '.png')\n",
        "    mask_ = cv2.resize(mask_, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:,:,:1]\n",
        "    Y_train[n] = mask_\n",
        "\n",
        "# test images\n",
        "X_test = np.zeros((len(testing_imgs), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "print('Resizing test images')\n",
        "for n, id_ in tqdm(enumerate(testing_imgs), total=len(testing_imgs)):\n",
        "  img = cv2.imread(data_folder + 'images/' + id_)\n",
        "  img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:,:,:IMG_CHANNELS]\n",
        "  X_test[n] = img"
      ],
      "metadata": {
        "id": "LBh3ZYeFhgWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN MODEL FIT TO TRAIN THE MODEL"
      ],
      "metadata": {
        "id": "VDGgbQgdh55k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api._v2.keras import callbacks\n",
        "#model Checkpoint\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint('Model_for_cracks.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
        "\n",
        "# callbacks = [\n",
        "#     tf.keras.callbacks.EarlyStopping(patience=4, monitor='val_loss'),\n",
        "#     tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
        "# ]\n",
        "\n",
        "results = new_model.fit(X_train, Y_train, validation_split=0.2, batch_size=10, epochs=15)"
      ],
      "metadata": {
        "id": "ynaF3k-IhwWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN NODEL PREDICTION ON TESTING DATA"
      ],
      "metadata": {
        "id": "w4y1SDjxiM4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_train = new_model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
        "preds_val = new_model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\n",
        "preds_test = new_model.predict(X_test, verbose=1)\n",
        "\n",
        "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
        "preds_val_t = (preds_val > 0.5).astype(np.uint8)\n",
        "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
      ],
      "metadata": {
        "id": "2749-iX2h3hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN MODEL EVALUATION ON TEST DATA"
      ],
      "metadata": {
        "id": "L0Sb3ivQiobk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing = np.zeros((len(testing_imgs), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\n",
        "\n",
        "test_results = new_model.evaluate(X_test, testing, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=\"logs/evaluate/\")])\n",
        "\n",
        "print(\"test loss, test acc: \", test_results)\n"
      ],
      "metadata": {
        "id": "4_UjQdOGiZg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mean_IoU = []\n",
        "con_matrix = []\n",
        "\n",
        "for n, id_ in tqdm(enumerate(testing_imgs), total=len(testing_imgs)):\n",
        "\n",
        "    img_mask = cv2.imread(data_folder + '/masks/' + testing_imgs[n][0:-4] + '.png')\n",
        "    true_mask = cv2.resize(img_mask, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:, :, :1]/255\n",
        "    true_mask_t = (true_mask > 0.4).astype(np.uint8)\n",
        "    testing[n] = true_mask_t\n",
        "\n",
        "    num_classes = 2\n",
        "    IOU_keras = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
        "    IOU_keras.update_state(true_mask_t, preds_test_t[n])\n",
        "    Mean_IoU.append(IOU_keras.result().numpy())\n",
        "\n",
        "\n",
        "    values = np.array(IOU_keras.get_weights()).reshape(num_classes, num_classes)\n",
        "    con_matrix.append(values)"
      ],
      "metadata": {
        "id": "L_gNU0r6ivnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model metrics\n",
        "\n",
        "Pr = []\n",
        "Re = []\n",
        "F1_score = []\n",
        "\n",
        "for matrix in con_matrix:\n",
        "\n",
        "    if (matrix[1, 1] + matrix[0, 1]) != 0:\n",
        "        precision = matrix[1, 1] / (matrix[1, 1] + matrix[0, 1])\n",
        "\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    if (matrix[1, 1] + matrix[1, 0]) != 0:\n",
        "        recall = matrix[1, 1] / (matrix[1, 1] + matrix[1, 0])\n",
        "\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    if (precision + recall) != 0:\n",
        "        F1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    else:\n",
        "        F1 = 0\n",
        "\n",
        "    Pr.append(precision)\n",
        "    Re.append(recall)\n",
        "    F1_score.append(F1)\n",
        "\n",
        "Ave_Pr = sum(Pr) / len(Pr)\n",
        "Ave_Re = sum(Re) / len(Re)\n",
        "Ave_F1 = sum(F1_score) / len(F1_score)\n",
        "\n",
        "print('Precision is: ', Ave_Pr)\n",
        "print('Recall is: ', Ave_Re)\n",
        "print('F1 Score is: ', Ave_F1)\n",
        "print('IoU is: ', sum(Mean_IoU) / len(Mean_IoU))"
      ],
      "metadata": {
        "id": "9hSnefB_i4RD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}