{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL THE REQUIRED PACKAGES"
      ],
      "metadata": {
        "id": "tnVB4-_aWHcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqrP-J7xV8d6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')pip install tensorflow\n",
        "\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from skimage.io import imread, imshow\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "print('packages installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DIVIDE YOUR IMAGE DATASET INTO TRAINING AND TESTING"
      ],
      "metadata": {
        "id": "fSSLG-GRWXy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder containing all the images and masks\n",
        "data_folder = '/Users/tebogoletshwiti/Desktop/Image_Segmentation'\n",
        "\n",
        "# List of images and masks\n",
        "# image_ids = next(os.walk(data_folder + '/images'))[2]\n",
        "masks_ids = next(os.walk(data_folder + '/masks'))[2]\n",
        "\n",
        "# divide images 80% to training and 20% to testing\n",
        "training_data = []\n",
        "testing_data = image_ids.copy()\n",
        "\n",
        "file1 = open(\"/content/gdrive/MyDrive/Colab Notebooks/Crack_Segmentation_Dataset-20231102T052005Z-001/Crack_Segmentation_Dataset/training_images.txt\",\"a\")\n",
        "file2 = open(\"/content/gdrive/MyDrive/Colab Notebooks/Crack_Segmentation_Dataset-20231102T052005Z-001/Crack_Segmentation_Dataset/testing_images.txt\",\"a\")\n",
        "\n",
        "\n",
        "total = 0\n",
        "while total <= (round(len(image_ids) * 0.8)):\n",
        "\n",
        "  index = random.randint(0, len(testing_data) - 1)\n",
        "  training_data.append(image_ids[index])\n",
        "  testing_data.pop(index)\n",
        "\n",
        "  total += 1\n",
        "\n",
        "for image in training_data:\n",
        "  file1.writelines(image + '\\n')\n",
        "\n",
        "for image in testing_data:\n",
        "  file2.writelines(image + '\\n')\n",
        "\n",
        "\n",
        "file1.close()\n",
        "file2.close()"
      ],
      "metadata": {
        "id": "v11Fd4PiWPMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ IMAGE NAMES FROM LIST CREATED ABOVE AND LOAD INTO LIST ITEM"
      ],
      "metadata": {
        "id": "NbRGs2tlWyY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_imgs = []\n",
        "testing_imgs = []\n",
        "\n",
        "data_folder = '/Users/tebogoletshwiti/Desktop/Image_Segmentation'\n",
        "\n",
        "file1 = open(data_folder + \"/training_images.txt\",\"r+\")\n",
        "file2 = open(data_folder + \"/testing_images.txt\",\"r+\")\n",
        "\n",
        "for item in file1.readlines():\n",
        "  training_imgs.append(item.strip())\n",
        "\n",
        "for item in file2.readlines():\n",
        "  testing_imgs.append(item.strip())\n",
        "\n",
        "mask_ids = training_imgs.copy()\n",
        "print(len(training_imgs))"
      ],
      "metadata": {
        "id": "zNCT5MuUWt5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATE YOUR TRAINING AND TESTING DATASETS TO LOAD INTO MODEL"
      ],
      "metadata": {
        "id": "RuAT2yYjXUTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 45\n",
        "np.random.seed = seed\n",
        "\n",
        "IMG_WIDTH = 640\n",
        "IMG_HEIGHT = 640\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "X_train = np.zeros((len(training_imgs), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_train = np.zeros((len(training_imgs), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "\n",
        "print('Resizing training images and masks')\n",
        "for n, id_ in tqdm(enumerate(training_imgs), total=len(training_imgs)):\n",
        "\n",
        "    img = cv2.imread(data_folder + '/images/' + id_)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:,:,:IMG_CHANNELS]\n",
        "    X_train[n] = img  #Fill empty X_train with values from img\n",
        "\n",
        "    mask_ = cv2.imread(data_folder + '/masks/' + id_[0:-4] + '.png')\n",
        "    mask_ = cv2.resize(mask_, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:,:,:1]\n",
        "    Y_train[n] = mask_\n",
        "\n",
        "# test images\n",
        "X_test = np.zeros((len(testing_imgs), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "sizes_test = []\n",
        "print('Resizing test images')\n",
        "for n, id_ in tqdm(enumerate(testing_imgs), total=len(testing_imgs)):\n",
        "  img = cv2.imread(data_folder + '/images/' + id_)\n",
        "  sizes_test.append([img.shape[0], img.shape[1]])\n",
        "  img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:,:,:IMG_CHANNELS]\n",
        "  X_test[n] = img\n",
        "\n",
        "print('Done!')\n",
        "\n",
        "image_x = random.randint(0, len(training_imgs))\n",
        "imshow(X_train[image_x])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[image_x]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JT4VS9SwXM9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BUILD THE MODEL"
      ],
      "metadata": {
        "id": "8eGSHWtbYOcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the model\n",
        "\n",
        "inputs = tf.keras.layers.Input((IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Rescaling(scale=1.0/255.0)(inputs)\n",
        "\n",
        "#contraction path\n",
        "c1 = tf.keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
        "c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "c1 = tf.keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "p1 = tf.keras.layers.MaxPool2D((2,2))(c1)\n",
        "\n",
        "c2 = tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "c2 = tf.keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "p2 = tf.keras.layers.MaxPool2D((2,2))(c2)\n",
        "\n",
        "c3 = tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "c3 = tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "p3 = tf.keras.layers.MaxPool2D((2,2))(c3)\n",
        "\n",
        "c4 = tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "c4 = tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "p4 = tf.keras.layers.MaxPool2D((2,2))(c4)\n",
        "\n",
        "c5 = tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "c5 = tf.keras.layers.Conv2D(256, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "#Expansive path\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2,2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2,2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2,2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0015)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wvCSNY6fX649"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN MODEL FIT TO TRAIN THE MODEL"
      ],
      "metadata": {
        "id": "ucSKZ12kYbdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api._v2.keras import callbacks\n",
        "#model Checkpoint\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint('Model_for_cracks.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=4, monitor='val_loss'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
        "]\n",
        "\n",
        "results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=15, epochs=40, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "h02bT_RHYTYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAKE PREDICTIONS ON TESTING AND SOME TRAINING DATA"
      ],
      "metadata": {
        "id": "eaLjHT6gZY_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
        "preds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\n",
        "preds_test = model.predict(X_test, verbose=1)\n",
        "\n",
        "#Convert predictions probabilities into 0s and 1s\n",
        "preds_train_t = (preds_train > 0.4).astype(np.uint8)\n",
        "preds_val_t = (preds_val > 0.4).astype(np.uint8)\n",
        "preds_test_t = (preds_test > 0.4).astype(np.uint8)"
      ],
      "metadata": {
        "id": "2bcotZ3VYeqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INPUT THE TESTING MASKS (ASSUMING NAMES OF IMAGES AND MASKS ARE THE SAME) AND CALCULATE INTERSECTION OVER UNION"
      ],
      "metadata": {
        "id": "wzqRVzOhbmWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing = np.zeros((len(testing_imgs), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\n",
        "\n",
        "Mean_IoU = []\n",
        "con_matrix = []\n",
        "\n",
        "for n, id_ in tqdm(enumerate(testing_imgs), total=len(testing_imgs)):\n",
        "    img_mask = cv2.imread(data_folder + '/masks/' + testing_imgs[n][0:-4] + '.png')\n",
        "    true_mask = cv2.resize(img_mask, (IMG_WIDTH, IMG_HEIGHT), interpolation = cv2.INTER_NEAREST)[:, :, :1]/255\n",
        "    true_mask_t = (true_mask > 0.5).astype(np.uint8)\n",
        "    testing[n] = true_mask_t\n",
        "\n",
        "    num_classes = 2\n",
        "    IOU_keras = tf.keras.metrics.MeanIoU(num_classes=num_classes)\n",
        "    IOU_keras.update_state(true_mask_t, preds_test_t[n])\n",
        "    Mean_IoU.append(IOU_keras.result().numpy())\n",
        "\n",
        "    values = np.array(IOU_keras.get_weights()).reshape(num_classes, num_classes)\n",
        "    con_matrix.append(values)"
      ],
      "metadata": {
        "id": "ELUKRmUrZfUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN MODEL EVALUATION ON YOUR TESTING DATA"
      ],
      "metadata": {
        "id": "3WorZfsAb_Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = model.evaluate(X_test, testing, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=\"logs/evaluate/\")])\n",
        "\n",
        "print(\"test loss, test acc: \", test_results)"
      ],
      "metadata": {
        "id": "r05p32b7bjom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CALCULATE MODEL METRICS OF PRECISION, RECALL, F1 SCORE, AND IoU"
      ],
      "metadata": {
        "id": "TJnycvrWc18c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model metrics\n",
        "\n",
        "Pr = []\n",
        "Re = []\n",
        "F1_score = []\n",
        "\n",
        "for matrix in con_matrix:\n",
        "\n",
        "    if (matrix[1, 1] + matrix[0, 1]) != 0:\n",
        "        precision = matrix[1, 1] / (matrix[1, 1] + matrix[0, 1])\n",
        "\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    if (matrix[1, 1] + matrix[1, 0]) != 0:\n",
        "        recall = matrix[1, 1] / (matrix[1, 1] + matrix[1, 0])\n",
        "\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    if (precision + recall) != 0:\n",
        "        F1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    else:\n",
        "        F1 = 0\n",
        "\n",
        "    Pr.append(precision)\n",
        "    Re.append(recall)\n",
        "    F1_score.append(F1)\n",
        "\n",
        "Ave_Pr = sum(Pr) / len(Pr)\n",
        "Ave_Re = sum(Re) / len(Re)\n",
        "Ave_F1 = sum(F1_score) / len(F1_score)\n",
        "\n",
        "print('Precision is: ', Ave_Pr)\n",
        "print('Recall is: ', Ave_Re)\n",
        "print('F1 Score is: ', Ave_F1)\n",
        "print('IoU is: ', sum(Mean_IoU) / len(Mean_IoU))"
      ],
      "metadata": {
        "id": "u3WVMvrtcOBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}